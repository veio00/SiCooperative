{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3263d4fd-8a3c-40d3-8f4c-1602939cd9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29fa314c-bf60-4818-9eb0-a45b7c1990a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "import os\n",
    "\n",
    "def run_notebook(notebook_path):\n",
    "    try:\n",
    "        with open(notebook_path) as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "        ep = ExecutePreprocessor(timeout=600, kernel_name='python3')\n",
    "        ep.preprocess(nb, {'metadata': {'path': os.path.dirname(notebook_path)}})\n",
    "\n",
    "        print(f\"Test {notebook_path} passed successfully. OK!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Test {notebook_path} failed: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcbfa7b8-50c7-475e-8518-2a3caeb3a37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 13:05:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".                                               \n",
      "Setting Spark log level to \"OFF\".                                               \n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".                                               \n",
      "Setting Spark log level to \"OFF\".                                               \n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".                                               \n",
      "Setting Spark log level to \"OFF\".                                               \n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".                                               \n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tests/test_extract.ipynb passed successfully. OK!\n"
     ]
    }
   ],
   "source": [
    "run_notebook(\"tests/test_extract.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3836d44a-e4f1-4bc4-b835-cca9dc79b67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting Spark log level to \"OFF\".\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Completo Python\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.LocalFileSystem\") \\\n",
    "    .config(\"spark.log.level\", \"OFF\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:mysql://mysql:3306/desafio?\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"sparkuser\",\n",
    "    \"password\": \"sparkpass\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab31fdfe-e239-4546-a56b-2f4625e7497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "associados = spark.read.jdbc(jdbc_url, \"associado\", properties=jdbc_properties)\n",
    "contas = spark.read.jdbc(jdbc_url, \"conta\", properties=jdbc_properties)\n",
    "cartoes = spark.read.jdbc(jdbc_url, \"cartao\", properties=jdbc_properties)\n",
    "movimentos = spark.read.jdbc(jdbc_url, \"movimento\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80fe67ed-3bd8-497f-96dc-8757c5c35f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "movimento_flat_join = associados \\\n",
    "    .join(contas, associados[\"id\"] == contas[\"id_associado\"], \"inner\") \\\n",
    "    .join(cartoes, (associados[\"id\"] == cartoes[\"id_associado\"]) & (contas[\"id\"] == cartoes[\"id_conta\"]), \"inner\") \\\n",
    "    .join(movimentos, cartoes[\"id\"] == movimentos[\"id_cartao\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "024627ac-3148-4430-8ed2-b79e73791972",
   "metadata": {},
   "outputs": [],
   "source": [
    "movimento_flat_columns = movimento_flat_join.select(\n",
    "    col(\"nome\").alias(\"nome_associado\"),\n",
    "    col(\"sobrenome\").alias(\"sobrenome_associado\"),\n",
    "    col(\"idade\").alias(\"idade_associado\"),\n",
    "    col(\"vlr_transacao\").alias(\"vlr_transacao_movimento\"),\n",
    "    col(\"des_transacao\").alias(\"des_transacao_movimento\"),\n",
    "    col(\"data_movimento\"),\n",
    "    col(\"num_cartao\").alias(\"numero_cartao\"),\n",
    "    col(\"nom_impresso\").alias(\"nome_impresso_cartao\"),\n",
    "    col(\"tipo_conta\"),\n",
    "    col(\"data_criacao\").alias(\"data_criacao_conta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9413260a-677c-4a15-b14a-22f21b855851",
   "metadata": {},
   "outputs": [],
   "source": [
    "movimento_flat = movimento_flat_columns.select([col(c).cast(\"string\").alias(c) for c in movimento_flat_columns.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6eb752e-4ecd-42e5-aa32-20ab4b3ecbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movimento_flat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a30ef6d-3cf0-432d-82f7-bd69d3207a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório atual: /notebooks\n",
      "Pasta criada e permissões ajustadas: /notebooks/csv\n",
      "Permissões de /notebooks/csv: 777\n",
      "Extraindo dados do DataFrame do Spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando CSV com Python puro em /notebooks/csv/movimento_flat_python.csv...\n",
      "CSV salvo com sucesso em /notebooks/csv/movimento_flat_python.csv\n",
      "ETL concluído! Arquivo salvo em /notebooks/csv/movimento_flat_python.csv\n"
     ]
    }
   ],
   "source": [
    "diretorio_atual = os.getcwd()\n",
    "print(f\"Diretório atual: {diretorio_atual}\")\n",
    "\n",
    "dir_destino = os.path.join(diretorio_atual, \"csv\")  # Cria /notebooks/csv\n",
    "nome = \"movimento_flat_python.csv\"\n",
    "caminho_final = os.path.join(dir_destino, nome)\n",
    "\n",
    "os.makedirs(dir_destino, exist_ok=True)\n",
    "os.chmod(dir_destino, 0o777)\n",
    "print(f\"Pasta criada e permissões ajustadas: {dir_destino}\")\n",
    "print(f\"Permissões de {dir_destino}: {oct(os.stat(dir_destino).st_mode)[-3:]}\")\n",
    "\n",
    "print(\"Extraindo dados do DataFrame do Spark...\")\n",
    "data = movimento_flat.collect()  # Traz os dados pro driver\n",
    "header = movimento_flat.columns  # Pega os nomes das colunas\n",
    "rows = [row.asDict() for row in data]  # Converte cada linha pra um dicionário\n",
    "\n",
    "print(f\"Salvando CSV com Python puro em {caminho_final}...\")\n",
    "with open(caminho_final, mode='w', newline='', encoding='ISO-8859-1') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=header)\n",
    "    writer.writeheader()  # Escreve o cabeçalho\n",
    "    for row in rows:\n",
    "        writer.writerow(row)  # Escreve cada linha\n",
    "print(f\"CSV salvo com sucesso em {caminho_final}\")\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "print(f\"ETL concluído! Arquivo salvo em {caminho_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cf6e58c4-54c1-41f5-95a3-e4c6d1c19e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 13:05:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tests/test_load.ipynb passed successfully. OK!\n"
     ]
    }
   ],
   "source": [
    "run_notebook(\"tests/test_load.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f599c8c-2a2f-44d1-94cf-55c6ab525823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".\n",
      "Setting Spark log level to \"OFF\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tests/test_transformation.ipynb passed successfully. OK!\n"
     ]
    }
   ],
   "source": [
    "run_notebook(\"tests/test_transformation.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fc6e00-1327-4a71-81b1-17bfbc4342b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
