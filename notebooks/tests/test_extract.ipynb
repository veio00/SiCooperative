{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be397e3-8aea-4b80-bcee-eec4ab3e0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "import csv\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5bbd15-c07f-49c9-9901-b1f85eff9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDados(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"Extract and Load Test\") \\\n",
    "            .master(\"spark://spark-master:7077\") \\\n",
    "            .config(\"spark.jars\", \"/opt/bitnami/spark/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "            .config(\"spark.log.level\", \"OFF\") \\\n",
    "            .getOrCreate()\n",
    "        self.jdbc_url = \"jdbc:mysql://mysql:3306/desafio?useUnicode=true&characterEncoding=UTF-8&useLegacyDatetimeCode=false&serverTimezone=UTC\"\n",
    "        self.jdbc_properties = {\n",
    "            \"user\": \"sparkuser\",\n",
    "            \"password\": \"sparkpass\",\n",
    "            \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "            \"useUnicode\": \"true\",\n",
    "            \"characterEncoding\": \"UTF-8\"\n",
    "        }\n",
    "\n",
    "    def tearDown(self):\n",
    "        if self.spark is not None:\n",
    "            self.spark.stop()\n",
    "            self.spark = None\n",
    "           \n",
    "    def test_associado_existe(self):\n",
    "        query = \"(SELECT 1 FROM information_schema.tables WHERE table_schema = 'desafio' AND table_name = 'associado') AS t\"\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, query, properties=self.jdbc_properties)\n",
    "        self.assertEqual(df.count(), 1)\n",
    "\n",
    "    def test_conta_existe(self):\n",
    "        query = \"(SELECT 1 FROM information_schema.tables WHERE table_schema = 'desafio' AND table_name = 'conta') AS t\"\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, query, properties=self.jdbc_properties)\n",
    "        self.assertEqual(df.count(), 1)\n",
    "\n",
    "    def test_cartao_existe(self):\n",
    "        query = \"(SELECT 1 FROM information_schema.tables WHERE table_schema = 'desafio' AND table_name = 'cartao') AS t\"\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, query, properties=self.jdbc_properties)\n",
    "        self.assertEqual(df.count(), 1)\n",
    "\n",
    "    def test_movimento_existe(self):\n",
    "        query = \"(SELECT 1 FROM information_schema.tables WHERE table_schema = 'desafio' AND table_name = 'movimento') AS t\"\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, query, properties=self.jdbc_properties)\n",
    "        self.assertEqual(df.count(), 1)\n",
    "\n",
    "    def test_associado_dados(self):\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, \"associado\", properties=self.jdbc_properties)\n",
    "        self.assertGreaterEqual(df.count(), 5)\n",
    "\n",
    "    def test_conta_dados(self):\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, \"conta\", properties=self.jdbc_properties)\n",
    "        self.assertGreaterEqual(df.count(), 5)\n",
    "\n",
    "    def test_cartao_dados(self):\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, \"cartao\", properties=self.jdbc_properties)\n",
    "        self.assertGreaterEqual(df.count(), 5)\n",
    "\n",
    "    def test_movimento_dados(self):\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, \"movimento\", properties=self.jdbc_properties)\n",
    "        self.assertGreaterEqual(df.count(), 5)\n",
    "\n",
    "    def test_associado_colunas(self):\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, \"associado\", properties=self.jdbc_properties)\n",
    "        colunas_esperadas = [\"id\", \"nome\", \"sobrenome\", \"idade\", \"email\"]\n",
    "        self.assertEqual(sorted(df.columns), sorted(colunas_esperadas))\n",
    "\n",
    "    def test_conta_colunas(self):\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, \"conta\", properties=self.jdbc_properties)\n",
    "        colunas_esperadas = [\"id\", \"tipo_conta\", \"data_criacao\", \"id_associado\"]\n",
    "        self.assertEqual(sorted(df.columns), sorted(colunas_esperadas))\n",
    "\n",
    "    def test_cartao_colunas(self):\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, \"cartao\", properties=self.jdbc_properties)\n",
    "        colunas_esperadas = [\"id\", \"num_cartao\", \"nom_impresso\", \"id_associado\", \"id_conta\"]\n",
    "        self.assertEqual(sorted(df.columns), sorted(colunas_esperadas))\n",
    "\n",
    "    def test_movimento_colunas(self):\n",
    "        df = self.spark.read.jdbc(self.jdbc_url, \"movimento\", properties=self.jdbc_properties)\n",
    "        colunas_esperadas = [\"id\", \"vlr_transacao\", \"des_transacao\", \"data_movimento\", \"id_cartao\"]\n",
    "        self.assertEqual(sorted(df.columns), sorted(colunas_esperadas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e304726-57b8-4f71-aa91-d3eb28120e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 15:28:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting Spark log level to \"OFF\".\n",
      ".Setting Spark log level to \"OFF\".\n",
      ".Setting Spark log level to \"OFF\".                                              \n",
      ".Setting Spark log level to \"OFF\".                                              \n",
      ".Setting Spark log level to \"OFF\".\n",
      ".Setting Spark log level to \"OFF\".                                              \n",
      ".Setting Spark log level to \"OFF\".                                              \n",
      ".Setting Spark log level to \"OFF\".\n",
      ".Setting Spark log level to \"OFF\".                                              \n",
      ".Setting Spark log level to \"OFF\".                                              \n",
      ".Setting Spark log level to \"OFF\".\n",
      ".Setting Spark log level to \"OFF\".                                              \n",
      ".                                                                               \n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 31.546s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=12 errors=0 failures=0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = unittest.TestLoader()\n",
    "suite = loader.loadTestsFromTestCase(TestDados)\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bb17e-4b2b-46e5-8100-75d14ef3a329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
