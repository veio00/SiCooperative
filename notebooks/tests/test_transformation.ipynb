{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7fb6e5-d315-440c-8662-d32cf13cc12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "import csv\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2d80e0-4f7a-4ffd-9e1b-48b49be74498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTransformations(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"Transformation Test\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .config(\"spark.log.level\", \"OFF\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        self.dir_destino = \"file:///notebooks/csv/\"\n",
    "        self.nome_python = \"movimento_flat_python.csv\"\n",
    "        self.nome_scala = \"movimento_flat_scala.csv\"\n",
    "        self.caminho_final_python = os.path.join(self.dir_destino, self.nome_python)\n",
    "        self.caminho_final_scala = os.path.join(self.dir_destino, self.nome_scala)\n",
    "\n",
    "    def tearDown(self):\n",
    "        if self.spark is not None:\n",
    "            self.spark.stop()\n",
    "            self.spark = None\n",
    "\n",
    "    def test_python_colunas_esperadas(self):\n",
    "        df = self.spark.read.option(\"header\", \"true\").csv(self.caminho_final_python)\n",
    "        colunas_esperadas = [\n",
    "            \"nome_associado\", \"sobrenome_associado\", \"idade_associado\",\n",
    "            \"vlr_transacao_movimento\", \"des_transacao_movimento\", \"data_movimento\",\n",
    "            \"numero_cartao\", \"nome_impresso_cartao\", \"tipo_conta\", \"data_criacao_conta\"\n",
    "        ]\n",
    "        self.assertEqual(sorted(df.columns), sorted(colunas_esperadas))\n",
    "\n",
    "    def test_scala_colunas_esperadas(self):\n",
    "        df = self.spark.read.option(\"header\", \"true\").csv(self.caminho_final_scala)\n",
    "        colunas_esperadas = [\n",
    "            \"nome_associado\", \"sobrenome_associado\", \"idade_associado\",\n",
    "            \"vlr_transacao_movimento\", \"des_transacao_movimento\", \"data_movimento\",\n",
    "            \"numero_cartao\", \"nome_impresso_cartao\", \"tipo_conta\", \"data_criacao_conta\"\n",
    "        ]\n",
    "        self.assertEqual(sorted(df.columns), sorted(colunas_esperadas))\n",
    "# \n",
    "    def test_python_tipos_dados(self):\n",
    "        df = self.spark.read.option(\"header\", \"true\").csv(self.caminho_final_python)\n",
    "        for col_name, dtype in df.dtypes:\n",
    "            self.assertEqual(dtype, \"string\")\n",
    "# \n",
    "    def test_scala_tipos_dados(self):\n",
    "        df = self.spark.read.option(\"header\", \"true\").csv(self.caminho_final_scala)\n",
    "        for col_name, dtype in df.dtypes:\n",
    "            self.assertEqual(dtype, \"string\")\n",
    "# \n",
    "    def test_python_valores_nulos(self):\n",
    "        df = self.spark.read.option(\"header\", \"true\").csv(self.caminho_final_python)\n",
    "        colunas_criticas = [\"nome_associado\", \"vlr_transacao_movimento\", \"numero_cartao\"]\n",
    "        for col in colunas_criticas:\n",
    "            nulos = df.filter(df[col].isNull()).count()\n",
    "            self.assertEqual(nulos, 0)\n",
    "# \n",
    "    def test_scala_valores_nulos(self):\n",
    "        df = self.spark.read.option(\"header\", \"true\").csv(self.caminho_final_scala)\n",
    "        colunas_criticas = [\"nome_associado\", \"vlr_transacao_movimento\", \"numero_cartao\"]\n",
    "        for col in colunas_criticas:\n",
    "            nulos = df.filter(df[col].isNull()).count()\n",
    "            self.assertEqual(nulos, 0)\n",
    " \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70676e7d-59d8-4202-a013-6a46f3c43a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Setting Spark log level to \"OFF\".\n",
      ".Setting Spark log level to \"OFF\".\n",
      ".Setting Spark log level to \"OFF\".\n",
      ".Setting Spark log level to \"OFF\".\n",
      ".Setting Spark log level to \"OFF\".\n",
      ".Setting Spark log level to \"OFF\".\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 10.114s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=6 errors=0 failures=0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = unittest.TestLoader()\n",
    "suite = loader.loadTestsFromTestCase(TestTransformations)\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f19904-c680-4ac8-819b-411e8b2697aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
